{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "\n",
    "<h1 align=center><font size = 5>CAPSTONE PROJECT</font></h1>\n",
    "<h2 align=center><font size = 5>AIML Certification Programme</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Student Name and ID:\n",
    "Mention your name and ID if done individually<br>\n",
    "If done as a group,clearly mention the contribution from each group member qualitatively and as a precentage.<br>\n",
    "1. KUNA MURALI (ID: 2024AIML030)                          \n",
    "\n",
    "2. MADIRE MAHESHKUMAR (ID: 2024AIML079)\n",
    "\n",
    "3. V VIJAY KUMAR (ID: 2024AIML100)\n",
    "\n",
    "4. GADIGA MOUNESWAR BABU (ID: 2024AIML095)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helmet Violation Detection from Indian CCTV Video\n",
    "\n",
    "**Problem statement:**\n",
    "    Detect and flag two-wheeler helmet violations (helmetless riding) from traffic camera frames in Indian cities in real-time.\n",
    "\n",
    "**Description:**\n",
    "Create a computer vision system using YOLOv8 and object tracking to detect two-wheeler riders and classify helmet usage. Optionally perform license plate OCR for enforcement.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "    •\tIndian Helmet Detection Dataset\n",
    "    \n",
    "    •\tResearch-generated dataset of Indian two-wheeler violations (images+video with annotations for helmet & plate) \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python==4.9.0.80\n",
    "!pip install matplotlib==3.8.4\n",
    "!pip install numpy==1.26.4\n",
    "!pip install pillow==10.3.0\n",
    "!pip install pandas==2.2.2\n",
    "!pip install seaborn==0.13.2\n",
    "!pip install scikit-learn==1.4.2\n",
    "!pip install torch==2.3.0\n",
    "!pip install notebook==7.2.0\n",
    "!pip install albumentations==1.4.8\n",
    "!pip install albucore==0.0.16\n",
    "!pip install ultralytics==8.0.134\n",
    "!pip install --upgrade ultralytics torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import hashlib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image, ImageDraw, ImageEnhance, ImageFilter\n",
    "import albumentations as A\n",
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "scripts_dir = os.path.abspath(os.path.join(os.getcwd(), '..', 'scripts'))\n",
    "sys.path.append(scripts_dir)\n",
    "for entry in os.listdir(scripts_dir):\n",
    "    entry_path = os.path.join(scripts_dir, entry)\n",
    "    if os.path.isdir(entry_path):\n",
    "        sys.path.append(entry_path)\n",
    "\n",
    "from utils import show_images_grid, split_and_copy_dataset, show_random_images_grid\n",
    "from flip import HorizontalFlip\n",
    "from zoom import DynamicZoomer\n",
    "from mosaic import MosaicAugmentor\n",
    "from cutout import CutoutAugmentor\n",
    "from synthetic import SyntheticImageAugmentor\n",
    "from edgedetect import EdgeDetectAugmentor\n",
    "from cutmix import CutMixAugmentor\n",
    "from rotate import RotateAugmentor\n",
    "from shadow import ShadowCastingAugmentor\n",
    "from grayscale import GrayscaleAugmentor\n",
    "from noise import NoiseInjectionAugmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"..\\\\data\\\\raw\\\\train\\\\images\"\n",
    "label_folder = \"..\\\\data\\\\raw\\\\train\\\\labels\"\n",
    "\n",
    "class_map = {\n",
    "    0: \"NumberPlate\",\n",
    "    1: \"Person\",\n",
    "    2: \"Helmet\",\n",
    "    3: \"Head\",\n",
    "    4: \"Motorbike\"\n",
    "}\n",
    "\n",
    "images = os.listdir(image_folder)\n",
    "labels = os.listdir(label_folder)\n",
    "\n",
    "\n",
    "print(\"Total images:\", len(images))\n",
    "print(\"Total label files:\", len(labels))\n",
    "print(\"Missing label files:\", set(os.path.splitext(i)[0] for i in images) - set(os.path.splitext(l)[0] for l in labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Horizontal Flipping** – Simulates helmets from different directions.\n",
    "\n",
    "* Horizontal flip in data augmentation is a technique that mirrors images along the vertical axis (left to right). \n",
    "* It effectively creates a flipped version of each image, doubling the variety of orientations the model sees during training. \n",
    "* This simple transformation helps models learn that objects and features can appear in different left-right positions, improving their generalization and robustness.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "\n",
    "* Horizontal flipping doubles the dataset size by creating mirrored versions of images.\n",
    "\n",
    "* Helps models generalize better by exposing them to left-right variations of objects, improving robustness, especially in tasks like object detection.\n",
    "\n",
    "* Ensures the bounding box labels remain accurate after flipping, which is critical for supervised learning correctness.\n",
    "\n",
    "* *his augmentation technique combats overfitting and enhances model performance on unseen test data by diversifying training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_image_folder = '..\\\\data\\\\processed\\\\train-flip\\\\images'\n",
    "flip_label_folder = '..\\\\data\\\\processed\\\\train-flip\\\\labels'\n",
    "\n",
    "flipper = HorizontalFlip(image_folder, label_folder, flip_image_folder, flip_label_folder)\n",
    "flipper.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(\n",
    "            [image_folder, flip_image_folder],\n",
    "            [label_folder, flip_label_folder],\n",
    "            ['013_16_jpeg.rf.cc06dc94c659d717549dc88f601c9ff1.jpg','013_16_jpeg.rf.cc06dc94c659d717549dc88f601c9ff1_flip.jpg'],\n",
    "            class_map=class_map\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(flip_image_folder, flip_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling/Zooming** – Randomly zoom in to focus on object of interest in the frame.\n",
    "\n",
    "* Image zoom augmentation randomly scales in on the image, cropping and resizing it back to the original size.\n",
    "* This simulates objects appearing larger or closer, helping the model learn to detect helmets and motorbikes at different scales and distances.\n",
    "* It improves robustness to camera zoom and real-world variations in object size.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "\n",
    "* Focuses Training on Relevant Objects:\n",
    "By zooming in tightly on objects (e.g., helmets), the model sees them larger and in more detail, which can improve detection accuracy, especially for small objects.\n",
    "\n",
    "* Increases Variation in Scale:\n",
    "Varying zoom levels mimic real-world scenarios where objects may be closer or farther.\n",
    "\n",
    "* Augments Dataset Without Changing Image Size:\n",
    "Keeps input size consistent with model expectations but changes spatial content.\n",
    "\n",
    "* Improves Model Robustness to Object Size Variability:\n",
    "Helps the model learn to detect objects at different scales and reduces bias toward object size distribution in original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoomed_image_folder = '..\\\\data\\\\processed\\\\train-zoom\\\\images'\n",
    "zoomed_label_folder = '..\\\\data\\\\processed\\\\train-zoom\\\\labels'\n",
    "\n",
    "zoomer = DynamicZoomer(image_folder, label_folder, zoomed_image_folder, zoomed_label_folder)\n",
    "zoomer.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(\n",
    "            [image_folder, zoomed_image_folder],\n",
    "            [label_folder, zoomed_label_folder],\n",
    "            ['013_48_jpeg.rf.dee0ddaded9e687ea455ad1ddb14d7fc.jpg','013_48_jpeg.rf.dee0ddaded9e687ea455ad1ddb14d7fc_zoom.jpg'],\n",
    "            class_map=class_map\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(zoomed_image_folder, zoomed_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rotation Data Augmentation** – Rotates images and their bounding boxes by a random angle (e.g., ±10–30 degrees). \n",
    "\n",
    "* This technique helps the model become invariant to the orientation of objects, simulating real-world scenarios where two-wheelers and helmets may not always appear perfectly upright in the frame. \n",
    "* Rotation augmentation improves robustness to camera tilt, road slope, and diverse viewpoints, leading to better generalization on unseen data.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "\n",
    "Rotation simulates viewpoint changes and varied camera angles, increasing model robustness to object orientation variance.\n",
    "\n",
    "* Albumentations handles proper geometric transformation of bounding boxes, preventing label mismatch.\n",
    "\n",
    "* Controlled rotation angles prevent unrealistic large rotations that would harm training.\n",
    "\n",
    "* Filling borders with a neutral color avoids visual artifacts near image edges.\n",
    "\n",
    "* Augmented data help models generalize better in real-world scenarios where objects/domain may be rotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_image_folder = '..\\\\data\\\\processed\\\\train-rotation\\\\images'     # Directory to save cutout images\n",
    "rotation_label_folder = '..\\\\data\\\\processed\\\\train-rotation\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "rotate_augmentor = RotateAugmentor(image_folder, label_folder, rotation_image_folder, rotation_label_folder)\n",
    "rotate_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(rotation_image_folder, rotation_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images_grid(\n",
    "            [image_folder, rotation_image_folder],\n",
    "            [label_folder, rotation_label_folder],\n",
    "            ['013_16_jpeg.rf.cc06dc94c659d717549dc88f601c9ff1.jpg','013_16_jpeg.rf.cc06dc94c659d717549dc88f601c9ff1_rotation.jpg'],\n",
    "            class_map=class_map\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding Box–Specific Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mosaic Augmentation** – Mosaic augmentation combines four different images into one by stitching them together in a grid.\n",
    "\n",
    "* This technique increases dataset diversity, helps the model learn to detect small objects, and improves robustness to varied object scales and contexts.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "* Combines Multiple Contexts:\n",
    "Mosaic augmentation merges four different scenes into one image, increasing contextual diversity and object co-occurrence variety.\n",
    "\n",
    "* Improves Small Object Detection:\n",
    "Resizing and grouping multiple images allows models to see varying object scales and densities.\n",
    "\n",
    "* Increases Dataset Variety:\n",
    "Creates many novel combinations from existing data, boosting effective dataset size without new data collection.\n",
    "\n",
    "* Helps Models Generalize:\n",
    "Exposes models to crowded or complex scenes, better preparing for real-world variability, especially useful in object detection tasks like helmet or number plate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic_image_folder = '..\\\\data\\\\processed\\\\train-mosaic\\\\images'     # Directory to save mirrored images\n",
    "mosaic_label_folder = '..\\\\data\\\\processed\\\\train-mosaic\\\\labels'  # Directory to save updated labels\n",
    "\n",
    "mosaic_augmentor = MosaicAugmentor(image_folder, label_folder, mosaic_image_folder, mosaic_label_folder)\n",
    "mosaic_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(mosaic_image_folder, mosaic_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CutMix Augmentation** – CutMix augmentation creates new training samples by cutting a patch from one image and pasting it onto another, while updating the bounding boxes accordingly. \n",
    "\n",
    "* This technique helps improve model robustness by exposing it to mixed-context images and encourages better generalization to occlusions and varied object arrangements.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "* Increases Data Diversity:\n",
    "Creates composite training samples by blending content from two images, enriching variability without collecting new data.\n",
    "\n",
    "* Improves Robustness to Occlusion:\n",
    "Simulates occlusions by partially replacing image areas with different objects/scenes, helping the model handle occluded or mixed environments.\n",
    "\n",
    "* Enhances Learning of Multiple Object Contexts:\n",
    "Mixes objects from two scenes, encouraging more generalizable feature representations.\n",
    "\n",
    "* Balances Class Distribution:\n",
    "Can increase examples of underrepresented classes by choosing pairs intelligently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutmix_image_folder = '..\\\\data\\\\processed\\\\train-cutmix\\\\images'     # Directory to save cutout images\n",
    "cutmix_label_folder = '..\\\\data\\\\processed\\\\train-cutmix\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "cutmix_augmentor = CutMixAugmentor(image_folder, label_folder, cutmix_image_folder, cutmix_label_folder)\n",
    "cutmix_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(cutmix_image_folder, cutmix_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cutout/Random Erasing** – Randomly mask parts of helmets or background for occlusion robustness.\n",
    "\n",
    "* Cutout data augmentation is a technique that randomly masks out (removes) a contiguous square region of an input image during training. \n",
    "* This masks a portion of the visual data, forcing the model to rely on less obvious or less prominent features to correctly recognize objects. By doing so, it improves the model's robustness to partial occlusions and over-reliance on specific image details.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "* Increases Robustness to Occlusion:\n",
    "Models learn to recognize objects despite missing or blocked parts by exposing them to images with random black regions.\n",
    "\n",
    "* Reduces Overfitting:\n",
    "By forcing reliance on multiple cues, the model generalizes better to unseen and partially occluded objects.\n",
    "\n",
    "* Easy to Implement and Efficient:\n",
    "Cutout is computationally inexpensive and can be applied online during training or preprocessing.\n",
    "\n",
    "* Improves Performance in Object Detection and Classification:\n",
    "Especially effective for scenarios where objects may be partially obstructed or appear in cluttered scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_image_folder = '..\\\\data\\\\processed\\\\train-cutout\\\\images'     # Directory to save cutout images\n",
    "cutout_label_folder = '..\\\\data\\\\processed\\\\train-cutout\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "cutout_augmentor = CutoutAugmentor(image_folder, label_folder, cutout_image_folder, cutout_label_folder)\n",
    "cutout_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(cutout_image_folder, cutout_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environmental Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthetic Data Augmentation Techniques:**\n",
    "\n",
    "- **Fog Simulation:** Adds artificial fog or haze to images, mimicking low-visibility conditions. This helps the model learn to detect objects in adverse weather, improving robustness to real-world foggy scenes.\n",
    "\n",
    "- **Rain Simulation:** Overlays rain streaks or droplets onto images, simulating rainy weather. This augmentation teaches the model to recognize helmets and vehicles even when visibility is reduced by rain.\n",
    "\n",
    "- **Blur Augmentation:** Applies motion blur or defocus blur to images, replicating camera shake or out-of-focus scenarios. This helps the model handle blurry frames from CCTV footage or fast-moving vehicles.\n",
    "\n",
    "- **Illumination Variation:** Adjusts brightness to simulate different lighting condition. This ensures the model is robust to varying illumination and can generalize across different times of day and lighting environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_image_folder = '..\\\\data\\\\processed\\\\train-synthetic\\\\images'     # Directory to save cutout images\n",
    "synthetic_label_folder = '..\\\\data\\\\processed\\\\train-synthetic\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "synthetic_augmentor = SyntheticImageAugmentor(image_folder, label_folder, synthetic_image_folder, synthetic_label_folder)\n",
    "synthetic_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(synthetic_image_folder, synthetic_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shadow Augmentation:** Adds synthetic shadows to simulate real-world conditions (vehicles passing next to flyover, under the foot over bridges)\n",
    "\n",
    "* It involves overlaying artificial shadows onto images to simulate real-world lighting conditions and occlusions. \n",
    "* This technique helps computer vision models, such as object detectors, learn to recognize and localize objects even when parts of the scene are darkened or partially obscured by shadows. \n",
    "* By introducing varying shapes, positions, and intensities of shadows, shadow augmentation improves a model's robustness to lighting variability and partial object occlusion, which are common challenges in real-world environments.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "* Simulates Realistic Environmental Shadows:\n",
    "Shadows frequently occur in outdoor scenes (e.g., streets, vehicles); augmenting images with shadows enhances model robustness to illumination variability.\n",
    "\n",
    "* Improves Generalization:\n",
    "Helps models learn invariant representations despite partial shadow occlusion of objects, reducing false negatives.\n",
    "\n",
    "* Adds Lighting Diversity Without Geometric Changes:\n",
    "Shadows alter pixel intensities without modifying spatial layout; labels remain valid.\n",
    "\n",
    "* Enhances Dataset Variability at Low Cost:\n",
    "Shadows are easy to generate programmatically by polygon overlays with varied shapes and intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_image_folder = '..\\\\data\\\\processed\\\\train-shadow\\\\images'     # Directory to save cutout images\n",
    "shadow_label_folder = '..\\\\data\\\\processed\\\\train-shadow\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "shadow_augmentor = ShadowCastingAugmentor(image_folder, label_folder, shadow_image_folder, shadow_label_folder)\n",
    "shadow_augmentor.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(shadow_image_folder, shadow_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photometric Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge Detection:** Highlights object boundaries and textures by converting images into edges\n",
    "\n",
    "* Edge detection in data augmentation is the process of transforming images by highlighting their boundaries and shape outlines using algorithms like Sobel or Canny filters. \n",
    "* This technique enriches the dataset with edge-enhanced images, helping models focus on the contours and structural features of objects rather than textures or colors. \n",
    "* Edge-based augmentation encourages the model to learn shape cues, improving its robustness to variations, occlusions, and noise, and is particularly valuable for tasks that depend on accurate object localization and boundary recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_image_folder = '..\\\\data\\\\processed\\\\train-edge\\\\images'     # Directory to save cutout images\n",
    "edge_label_folder = '..\\\\data\\\\processed\\\\train-edge\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "edge_augmentor = EdgeDetectAugmentor(image_folder, label_folder, edge_image_folder, edge_label_folder)\n",
    "edge_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(edge_image_folder, edge_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Greyscale Conversion:** Color transformation - removes color information, keeping only intensity\n",
    "\n",
    "* It refers to converting color images to greyscale as part of the training process. \n",
    "* This technique encourages models to focus on texture, shape, and structural features rather than color information. \n",
    "* By exposing models to greyscale versions of images, it enhances robustness against varying lighting conditions and color distortions, helping the model perform better when color cues are unreliable or missing.\n",
    "\n",
    "<b>Benefits</b>\n",
    "* Simulates Variations in Illumination and Color:\n",
    "Training on grayscale images forces models to rely more on texture, shape, and edge information, improving generalization when color cues are unavailable or misleading.\n",
    "\n",
    "* Improves Robustness Across Domains:\n",
    "Models become less sensitive to color distribution bias, helpful in scenarios where input images vary widely in color characteristics (e.g., night/day or different cameras).\n",
    "\n",
    "* Simple yet Effective Augmentation:\n",
    "Grayscale conversion is computationally efficient and easy to apply as a part of broader augmentation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale_image_folder = '..\\\\data\\\\processed\\\\train-grayscale\\\\images'     # Directory to save cutout images\n",
    "grayscale_label_folder = '..\\\\data\\\\processed\\\\train-grayscale\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "grayscale_augmentor = GrayscaleAugmentor(image_folder, label_folder, grayscale_image_folder, grayscale_label_folder)\n",
    "grayscale_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(grayscale_image_folder, grayscale_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Noise Augmentation:** Random noise is deliberately added to training data \n",
    "\n",
    "* Gaussian noise augmentation introduces random variations in pixel intensity following a normal distribution, which simulates sensor or environmental noise. \n",
    "* Salt and pepper noise augmentation randomly sets some pixels to black or white, mimicking impulse noise found in low-quality or corrupted images. \n",
    "* Both techniques help models become more robust to noisy, low-quality, or imperfect real-world data by training them on visually challenging samples, making predictions more reliable under non-ideal imaging conditions.\n",
    "\n",
    "<b>Benefits:</b>\n",
    "* Simulates Realistic Sensor Noise:\n",
    "Noise injection mimics common real-world artifacts like sensor noise, low light grain, or transmission errors, making models robust to noisy inputs.\n",
    "\n",
    "* Improves Model Generalization:\n",
    "Forces models to learn more discriminative features by reducing reliance on exact pixel patterns corrupted by noise.\n",
    "\n",
    "* Encourages Robust Feature Extraction:\n",
    "Models trained on noisy data handle noisy or corrupted images better during inference.\n",
    "\n",
    "* Simple and Fast to Implement:\n",
    "Adds minimal computational overhead and can be used as an offline or online augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_image_folder = '..\\\\data\\\\processed\\\\train-gaussian\\\\images'     # Directory to save cutout images\n",
    "gaussian_label_folder = '..\\\\data\\\\processed\\\\train-gaussian\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "gaussian_noise_augmentor = NoiseInjectionAugmentor(image_folder, label_folder, gaussian_image_folder, gaussian_label_folder, noise_type='gaussian')\n",
    "gaussian_noise_augmentor.process()\n",
    "\n",
    "salt_pepper_image_folder = '..\\\\data\\\\processed\\\\train-salt_pepper\\\\images'     # Directory to save cutout images\n",
    "salt_pepper_label_folder = '..\\\\data\\\\processed\\\\train-salt_pepper\\\\labels'  # Directory to save cutout labels\n",
    "\n",
    "salt_pepper_noise_augmentor = NoiseInjectionAugmentor(image_folder, label_folder, salt_pepper_image_folder, salt_pepper_label_folder, noise_type='salt_pepper')\n",
    "salt_pepper_noise_augmentor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(gaussian_image_folder, gaussian_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_random_images_grid(salt_pepper_image_folder, salt_pepper_label_folder, class_map, N=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Splitting and Copying for Train/Validation Sets\n",
    "This facilitate splitting image datasets into training and validation sets and organizing them into proper directory structures. These functions support working with datasets having corresponding label files in YOLO format.\n",
    "\n",
    "**split_and_copy_dataset**\n",
    "\n",
    "This function performs a straightforward split of a dataset into training and validation subsets based on a specified ratio, then copies images and corresponding label files to destination folders.\n",
    "\n",
    "**Workflow**:\n",
    "\n",
    "Deletes any existing data in destination folders for a clean start.\n",
    "\n",
    "Reads all images in source folder, shuffles them randomly.\n",
    "\n",
    "Splits shuffled images into training and validation sets per split_ratio.\n",
    "\n",
    "Copies images and associated label .txt files to train/val folders accordingly.\n",
    "\n",
    "Prints the count of images copied to each subset.\n",
    "\n",
    "**split_and_copy_all_processed**\n",
    "\n",
    "This function extends the above with support for multiple augmentation subfolders inside a processed root directory. It samples a fraction of images from each augmentation folder, then splits and copies them similarly.\n",
    "\n",
    "**Workflow**:\n",
    "\n",
    "Iterates over augmentation folders in processed_root.\n",
    "\n",
    "For each augmentation, samples a fraction (sample_ratio) of images randomly.\n",
    "\n",
    "Splits sampled images into train and val sets per split_ratio.\n",
    "\n",
    "Copies sampled, split images and labels to destination folders under model/train/<aug_type> and model/val/<aug_type>.\n",
    "\n",
    "Removes train- prefix in folder names before copy.\n",
    "\n",
    "Prints processed folder and counts per split.\n",
    "\n",
    "Cleans and recreates destination directories before copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_copy_dataset(\n",
    "    src_img_dir,\n",
    "    src_lbl_dir,\n",
    "    dst_train_img_dir,\n",
    "    dst_train_lbl_dir,\n",
    "    dst_val_img_dir,\n",
    "    dst_val_lbl_dir,\n",
    "    split_ratio=0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    Splits images and labels into train/val sets and copies them to destination folders.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dst_train_img_dir):\n",
    "        shutil.rmtree(dst_train_img_dir)\n",
    "    if os.path.exists(dst_train_lbl_dir):\n",
    "        shutil.rmtree(dst_train_lbl_dir)\n",
    "    if os.path.exists(dst_val_img_dir):\n",
    "        shutil.rmtree(dst_val_img_dir)\n",
    "    if os.path.exists(dst_val_lbl_dir):\n",
    "        shutil.rmtree(dst_val_lbl_dir)\n",
    "    os.makedirs(dst_train_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_train_lbl_dir, exist_ok=True)\n",
    "    os.makedirs(dst_val_img_dir, exist_ok=True)\n",
    "    os.makedirs(dst_val_lbl_dir, exist_ok=True)\n",
    "\n",
    "    img_files = [f for f in os.listdir(src_img_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "    random.shuffle(img_files)\n",
    "    split_idx = int(len(img_files) * split_ratio)\n",
    "    train_files = img_files[:split_idx]\n",
    "    val_files = img_files[split_idx:]\n",
    "\n",
    "    def copy_files(file_list, img_dst, lbl_dst):\n",
    "        for img_file in file_list:\n",
    "            img_src_path = os.path.join(src_img_dir, img_file)\n",
    "            lbl_src_path = os.path.join(src_lbl_dir, os.path.splitext(img_file)[0] + '.txt')\n",
    "            shutil.copy2(img_src_path, os.path.join(img_dst, img_file))\n",
    "            if os.path.exists(lbl_src_path):\n",
    "                shutil.copy2(lbl_src_path, os.path.join(lbl_dst, os.path.splitext(img_file)[0] + '.txt'))\n",
    "\n",
    "    copy_files(train_files, dst_train_img_dir, dst_train_lbl_dir)\n",
    "    copy_files(val_files, dst_val_img_dir, dst_val_lbl_dir)\n",
    "    print(f\"Copied {len(train_files)} images to train and {len(val_files)} images to val folders.\")\n",
    "\n",
    "def split_and_copy_all_processed(processed_root, dst_model_root, split_ratio=0.9, sample_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Dynamically go through all augmentation folders in processed, randomly select sample_ratio of images,\n",
    "    then split and copy to model/train and model/val. Removes 'train-' prefix from destination folder names.\n",
    "    \"\"\"\n",
    "    for aug_type in os.listdir(processed_root):\n",
    "        aug_img_dir = os.path.join(processed_root, aug_type, 'images')\n",
    "        aug_lbl_dir = os.path.join(processed_root, aug_type, 'labels')\n",
    "        if not (os.path.isdir(aug_img_dir) and os.path.isdir(aug_lbl_dir)):\n",
    "            continue\n",
    "        dst_folder = aug_type.replace('train-', '')\n",
    "        dst_train_img = os.path.join(dst_model_root, 'train', dst_folder, 'images')\n",
    "        dst_train_lbl = os.path.join(dst_model_root, 'train', dst_folder, 'labels')\n",
    "        dst_val_img = os.path.join(dst_model_root, 'val', dst_folder, 'images')\n",
    "        dst_val_lbl = os.path.join(dst_model_root, 'val', dst_folder, 'labels')\n",
    "\n",
    "        # Select a random sample of images\n",
    "        img_files = [f for f in os.listdir(aug_img_dir) if f.lower().endswith(('.jpg', '.png'))]\n",
    "        sample_size = max(1, int(len(img_files) * sample_ratio))\n",
    "        sampled_files = random.sample(img_files, sample_size)\n",
    "\n",
    "        # Split sampled files into train/val\n",
    "        random.shuffle(sampled_files)\n",
    "        split_idx = int(len(sampled_files) * split_ratio)\n",
    "        train_files = sampled_files[:split_idx]\n",
    "        val_files = sampled_files[split_idx:]\n",
    "\n",
    "        def copy_files(file_list, img_dst, lbl_dst):\n",
    "            # Delete folders if they exist, then create again\n",
    "            if os.path.exists(img_dst):\n",
    "                shutil.rmtree(img_dst)\n",
    "            if os.path.exists(lbl_dst):\n",
    "                shutil.rmtree(lbl_dst)\n",
    "            os.makedirs(img_dst, exist_ok=True)\n",
    "            os.makedirs(lbl_dst, exist_ok=True)\n",
    "            for img_file in file_list:\n",
    "                img_src_path = os.path.join(aug_img_dir, img_file)\n",
    "                lbl_src_path = os.path.join(aug_lbl_dir, os.path.splitext(img_file)[0] + '.txt')\n",
    "                shutil.copy2(img_src_path, os.path.join(img_dst, img_file))\n",
    "                if os.path.exists(lbl_src_path):\n",
    "                    shutil.copy2(lbl_src_path, os.path.join(lbl_dst, os.path.splitext(img_file)[0] + '.txt'))\n",
    "\n",
    "        copy_files(train_files, dst_train_img, dst_train_lbl)\n",
    "        copy_files(val_files, dst_val_img, dst_val_lbl)\n",
    "        print(f\"Processed augmentation: {dst_folder} | Train: {len(train_files)} | Val: {len(val_files)}\")\n",
    "\n",
    "# Example usage:\n",
    "split_and_copy_dataset(\n",
    "    src_img_dir='../data/raw/train/images',\n",
    "    src_lbl_dir='../data/raw/train/labels',\n",
    "    dst_train_img_dir='../data/model/train/raw/images',\n",
    "    dst_train_lbl_dir='../data/model/train/raw/labels',\n",
    "    dst_val_img_dir='../data/model/val/raw/images',\n",
    "    dst_val_lbl_dir='../data/model/val/raw/labels',\n",
    "    split_ratio=0.9)\n",
    "\n",
    "# Example usage:\n",
    "split_and_copy_all_processed('../data/processed', '../data/model', split_ratio=0.9, sample_ratio=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Object Detection Results on Test Images\n",
    "This code snippet demonstrates how to visualize YOLOv8 model predictions alongside ground truth annotations on a sample of test images.\n",
    "\n",
    "**Workflow**\n",
    "1.  **Load Pretrained Model**\n",
    "The YOLOv8 model is loaded from saved weights located in the project directory (best.pt).\n",
    "\n",
    "2. **Prepare Test Data Paths**\n",
    "Paths to test images and their corresponding label files (in YOLO format) are specified.\n",
    "\n",
    "3. **Random Sampling of Test Images**\n",
    "A configurable number (num_images) of test images are randomly selected to display.\n",
    "\n",
    "4. **Color Coding for Bounding Boxes**\n",
    "A predefined set of RGB colors assigns unique colors to each object class for easy distinction during visualization.\n",
    "\n",
    "**Benefits**\n",
    "Allows side-by-side comparison of model output with ground truth to assess detection quality visually.\n",
    "\n",
    "Color-coded bounding boxes enable easy distinction between different object classes.\n",
    "\n",
    "Random sampling helps to get an unbiased view of model performance over the test set.\n",
    "\n",
    "**Usage**\n",
    "Modify num_images to control how many test images are visualized in each run. Ensure test image and label directories are correctly set to the dataset locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load model\n",
    "model = YOLO('../runs/train/motorbike_yolov8s/weights/best.pt')\n",
    "\n",
    "# Path to test images and label files\n",
    "test_img_dir = '../data/raw/test/images'\n",
    "test_label_dir = '../data/raw/test/labels'\n",
    "\n",
    "# Configure number of images to display\n",
    "num_images = 10\n",
    "\n",
    "img_files = [f for f in os.listdir(test_img_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "num_images = min(num_images, len(img_files))  # Avoid exceeding available images\n",
    "selected_files = random.sample(img_files, num_images)\n",
    "\n",
    "colors = {0: (255, 0, 0), 1: (0, 255, 0), 2: (0, 255, 255), 3: (255, 165, 0), 4: (0, 0, 255)}\n",
    "\n",
    "def draw_boxes(image_path, boxes, class_ids=None):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = colors[class_ids[i]] if class_ids is not None else (255, 0, 0)\n",
    "        draw.rectangle(box, outline=color, width=2)\n",
    "    return img\n",
    "\n",
    "def read_label_file(label_file, image_path):\n",
    "    boxes = []\n",
    "    class_ids = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            class_id = int(parts[0])\n",
    "            x_center, y_center, w, h = map(float, parts[1:])\n",
    "            img_w, img_h = Image.open(image_path).size\n",
    "            x1 = (x_center - w/2) * img_w\n",
    "            y1 = (y_center - h/2) * img_h\n",
    "            x2 = (x_center + w/2) * img_w\n",
    "            y2 = (y_center + h/2) * img_h\n",
    "            boxes.append((x1, y1, x2, y2))\n",
    "            class_ids.append(class_id)\n",
    "    return boxes, class_ids\n",
    "\n",
    "fig, axes = plt.subplots(num_images, 3, figsize=(15, num_images*5))\n",
    "\n",
    "for i, img_file in enumerate(selected_files):\n",
    "    img_path = os.path.join(test_img_dir, img_file)\n",
    "    label_path = os.path.join(test_label_dir, img_file.rsplit('.', 1)[0] + '.txt')\n",
    "\n",
    "    # Original Image\n",
    "    orig_img = Image.open(img_path).convert(\"RGB\")\n",
    "    axes[i, 0].imshow(orig_img)\n",
    "    axes[i, 0].set_title(\"Original Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Ground Truth\n",
    "    gt_boxes, gt_classes = read_label_file(label_path, img_path)\n",
    "    img_gt = draw_boxes(img_path, gt_boxes, class_ids=gt_classes)\n",
    "    axes[i, 1].imshow(img_gt)\n",
    "    axes[i, 1].set_title(\"Ground Truth\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    # Model Prediction\n",
    "    results = model(img_path)\n",
    "    boxes_pred = results[0].boxes.xyxy.cpu().numpy()\n",
    "    class_ids_pred = results[0].boxes.cls.cpu().numpy().astype(int)\n",
    "    img_pred = draw_boxes(img_path, boxes_pred, class_ids=class_ids_pred)\n",
    "    axes[i, 2].imshow(img_pred)\n",
    "    axes[i, 2].set_title(\"Model Prediction\")\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
